Notes to utilize Mask Recognition System:

We are to stabilize and finalize the current system before implementing DAG below.


Based on the provided sources, the current project flow **does not utilize a Directed Acyclic Graph (DAG)**. Instead, the codebase is structured as a **modular, synchronous, and procedural pipeline**. The execution follows a fixed linear sequence where each component is called manually within a main processing loop.

### Analysis of the Current Flow
The current execution logic in `MultiSourceRealTimeProcessor` and `FaceRecognitionSystem` is strictly sequential:
1.  **Frame Acquisition:** Capturing frames from multiple sources via `StreamManager`.
2.  **Pre-processing:** Resizing and validating the frame for processing.
3.  **The Recognition Pipeline:** A single method `process_frame` executes four sub-steps in order: Face Detection → Mask Detection → Embedding Extraction → Face Recognition.
4.  **Tracking:** Passing results to the `TrackingManager` to assign stable IDs.
5.  **Post-processing/Action:** Handling identity policies, triggering alerts, and logging violations.

While this is modular, it is "brittle" in terms of data flow; if one part of the sequence changes, the hard-coded logic in the processor must be manually refactored.

---

### Proposing a Basic DAG Implementation
To transition the current codebase toward a DAG-based flow from a basic level, you can implement a **Task-Dependency Model**. This involves wrapping existing modular components into "Nodes" and defining "Edges" (dependencies) between them.

#### 1. Define the Task Abstraction (The Nodes)
Create a base `Node` class that wraps the existing system components. Each node will represent a functional block currently found in the `processing` or `streaming` directories.
*   **CaptureNode:** Wraps `StreamManager.get_frame()`.
*   **DetectionNode:** Wraps `FaceDetectionSystem.detect_faces()`.
*   **MaskNode:** Wraps `FaceRecognitionSystem.detect_mask()`.
*   **TrackingNode:** Wraps `TrackingManager.process_frame()`.
*   **AlertNode:** Wraps `DurationAwareAlertManager.process_frame_violations()`.

#### 2. Map the Dependencies (The Edges)
Instead of calling methods sequentially in a loop, you define which tasks must complete before others start. A basic DAG for this project would look like this:
*   **Level 1:** `CaptureNode` (No dependencies).
*   **Level 2:** `DetectionNode` (Depends on `CaptureNode`).
*   **Level 3 (Parallel):** `MaskNode` and `ExtractionNode` both depend on `DetectionNode`.
*   **Level 4:** `RecognitionNode` (Depends on `ExtractionNode`).
*   **Level 5:** `TrackingNode` (Depends on `RecognitionNode` and `MaskNode`).
*   **Level 6 (Parallel):** `AlertNode` and `LoggingNode` both depend on `TrackingNode`.

#### 3. Implement a Basic Graph Runner
At the lowest level of implementation, you would replace the current `while self.running` loop in `run_multi_source_stable` with a **Topological Sort Runner**.

```python
# Conceptual Basic Implementation (not currently in sources)
class SimpleGraphRunner:
    def __init__(self):
        self.nodes = {}
        self.results = {}

    def add_node(self, name, task_func, dependencies=[]):
        self.nodes[name] = {'func': task_func, 'deps': dependencies}

    def execute(self, input_data):
        # Executes tasks in an order that respects dependencies
        # This replaces the hard-coded sequence in multi_realtime.py
        pass
```

### Benefits of this Transition
*   **Parallelism:** Currently, mask detection and embedding extraction happen one after another. In a DAG, these could be dispatched to separate GPU streams or threads simultaneously since they both only depend on the detected face ROI.
*   **Conditional Execution:** You could easily configure the graph to skip the `RecognitionNode` if the `FaceQualityAssessor` determines the face quality is below the `min_face_quality` threshold, without nesting complex `if/else` logic in the main loop.
*   **Scalability:** Adding new features (like scene analysis) becomes a matter of adding a new node to the graph and linking it to the `CaptureNode`, rather than modifying the core processing method.

**Analogy:**
Think of the current project like a **traditional assembly line** where a car must pass every single station in a straight line, even if a station isn't needed. Transitioning to a **DAG** is like an **automated workshop** where different robotic arms work on different parts of the car at the same time, and parts only move to the next station once their specific prerequisites are finished.


If the current codebase were transitioned from its sequential procedural structure to a **Directed Acyclic Graph (DAG)** execution model, the following enhancements would significantly improve the system's performance, scalability, and intelligence:

### 1. True Concurrent Branching (Parallelism)
Currently, the `process_frame` pipeline executes sub-tasks one after another: Face Detection $\rightarrow$ Mask Detection $\rightarrow$ Embedding Extraction $\rightarrow$ Face Recognition. 
*   **Enhancement:** In a DAG, once the **FaceDetectionSystem** identifies a face ROI, the **MaskDetection** and **EmbeddingExtraction** tasks could be dispatched as **parallel branches**. Since mask detection depends only on the pixel data of the ROI and does not require the face embedding, these can run simultaneously on different CPU threads or GPU streams.
*   **Multi-Source Parallelism:** The current loop processes each camera source serially. A DAG would allow the **CaptureNode** for "Camera 1" and "Camera 2" to trigger independent processing sub-graphs, maximizing the utilization of multi-core processors.

### 2. Intelligent Short-Circuiting (Computational Efficiency)
The existing logic uses manual `if/else` checks to decide whether to skip steps like multi-scale processing.
*   **Enhancement:** A DAG allows for **Conditional Edges**. For example, a **QualityGateNode** could analyze the ROI. If the `overall_quality` is below the `min_face_quality` threshold, the graph would "short-circuit," bypass the expensive **VoyagerRecognitionNode**, and move directly to a **TrackingNode** with an "Unknown" identity label.
*   **Adaptive Paths:** If the `SceneContextAnalyzer` detects high motion, the DAG could dynamically route the frame through a "Fast-Path" node that uses lower resolution and skips **TemporalFusion** to maintain real-time FPS.

### 3. Asynchronous Pipelining (Reduced Latency)
The procedural loop currently waits for a frame to be fully processed before capturing the next one from the same source.
*   **Enhancement:** Utilizing a DAG with **Buffer Edges** enables a "Producer-Consumer" pipeline. The **StreamManager** node can continuously push frames into a queue while the **DetectionNode** pulls from it asynchronously. This ensures the camera buffer never overflows and the GPU is never idle, effectively decoupling the **Capture Rate** from the **Processing Rate**.

### 4. Modular Feature Injection (Scalability)
Adding new features like `SceneContextAnalyzer` or `FairnessController` currently requires manual insertion into the core processing methods.
*   **Enhancement:** With a DAG, the system becomes a **Plugin Architecture**. If you want to add a "Helmet Detection" or "Uniform Check" module, you simply add a new **Node** and link its input to the **DetectionNode**. You would not need to modify `MultiSourceRealTimeProcessor` or `RobustFaceRecognitionSystem` logic at all.

### 5. Priority-Based Resource Allocation
The system currently treats all sources and all faces with equal processing weight unless manually configured.
*   **Enhancement:** A **SchedulerNode** could manage the DAG's execution. Based on the `priority` of a source (e.g., a "high" priority entrance camera vs. a "low" priority hallway camera), the scheduler could allocate more workers to the high-priority DAG paths. If the system detects high `memory_mb` usage, the scheduler can temporarily disable non-critical DAG nodes like **ImageLogging** for lower-priority sources.

### Summary of DAG-Enabled Logic Flow
| Current Sequential Flow | Proposed DAG Enhancement |
| :--- | :--- |
| **Serial:** Detection $\rightarrow$ Mask $\rightarrow$ Embedding | **Parallel:** Mask & Embedding run together after Detection. |
| **Manual Checks:** Code-level `if` statements for quality. | **Dynamic Routing:** Graph logic handles skips automatically. |
| **Blocking:** Loop waits for detection to finish before capture. | **Pipelining:** Nodes process different frames at different stages. |
| **Rigid:** Adding a feature requires refactoring `process_frame`. | **Modular:** Adding a feature is a new node entry in the graph. |

**Analogy:**
The current project is like a **single chef** cooking a 5-course meal alone—he must finish the soup before he can even start chopping vegetables for the salad. A **DAG-enhanced system** is like a **professional kitchen staff**: while one person is plating the soup (Output), another is grilling the steak (Recognition), and a third is already washing the vegetables for the next order (Capture). Parts of the meal move forward as soon as their specific prep is done, making the whole kitchen significantly faster and more organized.

